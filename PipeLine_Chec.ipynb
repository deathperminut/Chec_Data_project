{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQ0x-sZF6Fi"
      },
      "source": [
        "## importamos las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6SR4i-zFz2J",
        "outputId": "6517bebe-f96d-4d7e-e24e-8a2235b1e22c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "# Importa pyarrow para el soporte Parquet\n",
        "import pyarrow.parquet as pq\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###MODELO\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D,Conv2DTranspose\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Flatten, Reshape\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRVJMiWrHUql"
      },
      "source": [
        "## MODELOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kkF1yKdHeH7"
      },
      "source": [
        "cargamos los modelos seleccionados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Read the json file with the credentials:\n",
        "with open(\"Archivo_de_Credenciales_escritura.json\", 'r') as archivo:\n",
        "    credenciales = json.load(archivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "msMdsstOg2Gm"
      },
      "outputs": [],
      "source": [
        "class reparametrize(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    FUNCIÓN PARA MUESTREAR EL ESPACIO LATENTE EN EL AUTOENCODER\n",
        "    VARIACIONAL Y DEFINIR LA FUNCIÓN DE COSTO CUSTOM\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        eta = tf.random.normal(tf.shape(log_var))\n",
        "        sigma = tf.math.exp(log_var / 2)\n",
        "        return  mean + sigma * eta\n",
        "\n",
        "\n",
        "class VaeModel():\n",
        "\n",
        "      def __init__(self,Credenciales):\n",
        "\n",
        "         self.generate_models()\n",
        "         self.Credenciales = Credenciales\n",
        "         self.init_server()\n",
        "\n",
        "      def init_server(self):\n",
        "         # Establecer la cadena de conexión para conectar a la base de datos\n",
        "          self.server = \"10.46.6.56\\CHECSQLDES\"\n",
        "          #database = 'DM_OPERACION'\n",
        "          self.username = self.Credenciales['username']\n",
        "          self.password = self.Credenciales['password']\n",
        "          self.driver = '{ODBC Driver 17 for SQL Server}'\n",
        "          self.database = 'DM_OPERACION'\n",
        "          self.conexion_str = f\"DRIVER={self.driver};SERVER={self.server};DATABASE={self.database};UID={self.username};PWD={self.password}\"\n",
        "\n",
        "\n",
        "      def VAE_LATENT_DIM_(self,Chans, Samples, dropoutRate = 0.5, l1 = 0, l2 = 0,latent_dim = 160):\n",
        "            \"\"\"\n",
        "            FUNCIÓN PARA GENERAR EL MODELO VAE\n",
        "            -----------------------------------\n",
        "            Chans = Número de canales\n",
        "            Samples = Muestras por trial\n",
        "            dropoutRate = HiperParametro para definir el dropout de las capas\n",
        "            l1, l2  = Parametro de regularización\n",
        "            \"\"\"\n",
        "\n",
        "            filters      = (1,40) ##ESTRUCTURA BASE DEL MODELO MULTITASK\n",
        "            strid        = (1,15) ##ESTRUCTURA BASE DEL MODELO MULTITASK\n",
        "            pool         = (1,75) ## ESTRUCTURA BASE DEL MODELO MULTITASK\n",
        "            bias_spatial = True ## QUE ES ESTE PARAMETRO?\n",
        "            ## ENCODER\n",
        "            input_main   = Input((Chans, Samples, 1))\n",
        "            block1       = Conv2D(latent_dim, filters, strides=(1,2),\n",
        "                                          input_shape=(Chans, Samples, 1),kernel_regularizer=l1_l2(l1=l1,l2=l2),\n",
        "                                          name='Conv2D_1',\n",
        "                                          kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
        "            block1       = Conv2D(latent_dim, (Chans, 1), use_bias=bias_spatial, kernel_regularizer=l1_l2(l1=l1,l2=l2),\n",
        "                                  name='Conv2D_2',\n",
        "                                  kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
        "            block1       = BatchNormalization(epsilon=1e-05, momentum=0.1)(block1)\n",
        "            Act1         = Activation('elu')(block1)\n",
        "            block1       = AveragePooling2D(pool_size=pool, strides=strid)(Act1)\n",
        "            block1       = Dropout(dropoutRate,name='bottleneck')(block1) ## ENCODER\n",
        "            ### MODELO PROBABILISTICO\n",
        "            mu           = Dense(latent_dim,name='mu')(block1) ##\n",
        "            log_var      = Dense(latent_dim,name='log_var')(block1)\n",
        "            codings      = reparametrize(name='Code')([mu, log_var]) ## CODIFICAMOS CON LA MEDIA Y VARIANZA DADA\n",
        "\n",
        "\n",
        "\n",
        "            ##CAPA DE DECODIFICACIÓN\n",
        "            block2       = Conv2DTranspose(latent_dim, pool,strides=strid,activation='tanh', kernel_regularizer=l1_l2(l1=l1,l2=l2),\n",
        "                                  kernel_constraint = max_norm(2., axis=(0,1,2)))(codings)\n",
        "            block2       = Resizing(block2.shape[1], Act1.shape[2])(block2)\n",
        "            block2       = Conv2DTranspose(latent_dim, (Chans, 1), use_bias=bias_spatial, kernel_regularizer=l1_l2(l1=l1,l2=l2),\n",
        "                                  kernel_constraint = max_norm(2., axis=(0,1,2)))(block2)\n",
        "            block2       = Conv2DTranspose(1, filters,strides=(1,2),\n",
        "                                          input_shape=(Chans, Samples, 1),kernel_regularizer=l1_l2(l1=l1,l2=l2),\n",
        "                                          kernel_constraint = max_norm(2., axis=(0,1,2)))(block2)\n",
        "\n",
        "            model = Model(inputs=input_main, outputs=[block2])\n",
        "\n",
        "            return model\n",
        "\n",
        "\n",
        "      def vae_loss(self,mu, log_var):\n",
        "            \"\"\"\n",
        "            función para definir la función del perdida del modelo VAE\n",
        "            \"\"\"\n",
        "            kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
        "\n",
        "            # Definir\n",
        "\n",
        "            # Suma de ambas pérdidas\n",
        "            total_loss = kl_loss\n",
        "\n",
        "            return total_loss\n",
        "\n",
        "      def generate_models(self):\n",
        "            \"\"\"\n",
        "            función para definir cada uno de los modelos cargados\n",
        "            con los pesos guardados\n",
        "            \"\"\"\n",
        "            #######################################\n",
        "            ################ 10 ###################\n",
        "            #######################################\n",
        "\n",
        "            Channels = 6\n",
        "            Samples = 672\n",
        "            self.MODEL_10 = self.VAE_LATENT_DIM_(Chans=Channels,Samples = Samples,latent_dim = 260) ## OBTENEMOS EL MODELO\n",
        "            # OBTENEMOS LAS ENTRADAS Y SALIDAS DEL MODELO\n",
        "            inputs = self.MODEL_10.input ##ENTRADA\n",
        "            outputs = self.MODEL_10.output ##SALIDAS [reconstrucción,clasificación]\n",
        "            # Obtener las capas de salida del espacio latente (mu y log_var)\n",
        "            mu_layer = self.MODEL_10.get_layer('mu')\n",
        "            log_var_layer = self.MODEL_10.get_layer('log_var')\n",
        "\n",
        "            # Obtener las salidas de las capas de mu y log_var\n",
        "            mu_output = mu_layer.output\n",
        "            log_var_output = log_var_layer.output\n",
        "\n",
        "            # Compile the model with the custom loss function\n",
        "            self.MODEL_10.add_loss(self.vae_loss(mu_output,log_var_output))\n",
        "            self.MODEL_10.compile(optimizer='adam',loss=['mse'])\n",
        "\n",
        "            ###CARGAMOS LOS PESOS\n",
        "            self.MODEL_10.load_weights('Pesos/modelo_10_Class_v1 (1) (1).h5')\n",
        "\n",
        "\n",
        "            #######################################\n",
        "            ###############  2  ###################\n",
        "            #######################################\n",
        "\n",
        "            self.MODEL_2 = self.VAE_LATENT_DIM_(Chans=Channels,Samples = Samples,latent_dim = 340) ## OBTENEMOS EL MODELO\n",
        "            # OBTENEMOS LAS ENTRADAS Y SALIDAS DEL MODELO\n",
        "            inputs = self.MODEL_2.input ##ENTRADA\n",
        "            outputs = self.MODEL_2.output ##SALIDAS [reconstrucción,clasificación]\n",
        "            # Obtener las capas de salida del espacio latente (mu y log_var)\n",
        "            mu_layer = self.MODEL_2.get_layer('mu')\n",
        "            log_var_layer = self.MODEL_2.get_layer('log_var')\n",
        "\n",
        "            # Obtener las salidas de las capas de mu y log_var\n",
        "            mu_output = mu_layer.output\n",
        "            log_var_output = log_var_layer.output\n",
        "\n",
        "            # Compile the model with the custom loss function\n",
        "            self.MODEL_2.add_loss(self.vae_loss(mu_output,log_var_output))\n",
        "            self.MODEL_2.compile(optimizer='adam',loss=['mse'])\n",
        "\n",
        "            ###CARGAMOS LOS PESOS\n",
        "            self.MODEL_2.load_weights('Pesos/model_2_340 (1) (1).h5')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #######################################\n",
        "            ###############  3  ###################\n",
        "            #######################################\n",
        "\n",
        "\n",
        "            self.MODEL_3 = self.VAE_LATENT_DIM_(Chans=Channels,Samples = Samples,latent_dim = 260) ## OBTENEMOS EL MODELO\n",
        "            # OBTENEMOS LAS ENTRADAS Y SALIDAS DEL MODELO\n",
        "            inputs = self.MODEL_3.input ##ENTRADA\n",
        "            outputs = self.MODEL_3.output ##SALIDAS [reconstrucción,clasificación]\n",
        "            # Obtener las capas de salida del espacio latente (mu y log_var)\n",
        "            mu_layer = self.MODEL_3.get_layer('mu')\n",
        "            log_var_layer = self.MODEL_3.get_layer('log_var')\n",
        "\n",
        "            # Obtener las salidas de las capas de mu y log_var\n",
        "            mu_output = mu_layer.output\n",
        "            log_var_output = log_var_layer.output\n",
        "\n",
        "            # Compile the model with the custom loss function\n",
        "            self.MODEL_3.add_loss(self.vae_loss(mu_output,log_var_output))\n",
        "            self.MODEL_3.compile(optimizer='adam',loss=['mse'])\n",
        "\n",
        "            ###CARGAMOS LOS PESOS\n",
        "            self.MODEL_3.load_weights('Pesos/modelo_3_Class_v1_260 (1).h5')\n",
        "\n",
        "\n",
        "            #######################################\n",
        "            ################ GC ###################\n",
        "            #######################################\n",
        "\n",
        "\n",
        "            self.MODEL_GOOD = self.VAE_LATENT_DIM_(Chans=Channels,Samples = Samples,latent_dim = 560) ## OBTENEMOS EL MODELO\n",
        "            # OBTENEMOS LAS ENTRADAS Y SALIDAS DEL MODELO\n",
        "            inputs = self.MODEL_GOOD.input ##ENTRADA\n",
        "            outputs = self.MODEL_GOOD.output ##SALIDAS [reconstrucción,clasificación]\n",
        "            # Obtener las capas de salida del espacio latente (mu y log_var)\n",
        "            mu_layer = self.MODEL_GOOD.get_layer('mu')\n",
        "            log_var_layer = self.MODEL_GOOD.get_layer('log_var')\n",
        "\n",
        "            # Obtener las salidas de las capas de mu y log_var\n",
        "            mu_output = mu_layer.output\n",
        "            log_var_output = log_var_layer.output\n",
        "\n",
        "            # Compile the model with the custom loss function\n",
        "            self.MODEL_GOOD.add_loss(self.vae_loss(mu_output,log_var_output))\n",
        "            self.MODEL_GOOD.compile(optimizer='adam',loss=['mse'])\n",
        "\n",
        "            ###CARGAMOS LOS PESOS\n",
        "            self.MODEL_GOOD.load_weights('Pesos/modelo_GOODClasses_560 (1).h5')\n",
        "\n",
        "\n",
        "            ###################################\n",
        "            ############## ALL ################\n",
        "            ###################################\n",
        "\n",
        "\n",
        "            self.MODEL_ALL = self.VAE_LATENT_DIM_(Chans=Channels,Samples = Samples,latent_dim = 560) ## OBTENEMOS EL MODELO\n",
        "            # OBTENEMOS LAS ENTRADAS Y SALIDAS DEL MODELO\n",
        "            inputs = self.MODEL_ALL.input ##ENTRADA\n",
        "            outputs = self.MODEL_ALL.output ##SALIDAS [reconstrucción,clasificación]\n",
        "            # Obtener las capas de salida del espacio latente (mu y log_var)\n",
        "            mu_layer = self.MODEL_ALL.get_layer('mu')\n",
        "            log_var_layer = self.MODEL_ALL.get_layer('log_var')\n",
        "\n",
        "            # Obtener las salidas de las capas de mu y log_var\n",
        "            mu_output = mu_layer.output\n",
        "            log_var_output = log_var_layer.output\n",
        "\n",
        "            # Compile the model with the custom loss function\n",
        "            self.MODEL_ALL.add_loss(self.vae_loss(mu_output,log_var_output))\n",
        "            self.MODEL_ALL.compile(optimizer='adam',loss=['mse'])\n",
        "\n",
        "            ###CARGAMOS LOS PESOS\n",
        "            self.MODEL_ALL.load_weights('Pesos/modelo_GOODClasses_560 (1).h5')\n",
        "\n",
        "\n",
        "\n",
        "      def model_iteration(self,df_circuits):\n",
        "\n",
        "            \"\"\"\n",
        "            función para iterar por cada serie de tiempo de cada circuito\n",
        "            \"\"\"\n",
        "            #### obtenemos los circuitos unicos del dataframe\n",
        "            circuits = df_circuits['CIRCUITO'].unique()\n",
        "\n",
        "            ### iteramos por cada uno de esos circuitos\n",
        "\n",
        "            for circuit in circuits:\n",
        "\n",
        "                ###### obtenemos el dataframe referente a dicho circuito\n",
        "\n",
        "                df_circuit = df_circuits.loc[df_circuits['CIRCUITO'] == circuit].sort_values(by='TIEMPO_AJUSTADO', ascending=True)\n",
        "\n",
        "                ###### segmentamos el dataframe por cada 672 datos e iteramos\n",
        "                self.iterar_por_grupos(df_circuit,672)\n",
        "\n",
        "\n",
        "      def iterar_por_grupos(self,dataframe, tamano_grupo):\n",
        "            # Obtener la cantidad total de filas en el DataFrame\n",
        "            total_filas = len(dataframe)\n",
        "\n",
        "            # Calcular la cantidad de grupos necesarios\n",
        "            cantidad_grupos = total_filas % tamano_grupo\n",
        "\n",
        "            if (cantidad_grupos == 0):\n",
        "               cantidad_grupos = (total_filas // tamano_grupo)\n",
        "            else:\n",
        "               cantidad_grupos = (total_filas // tamano_grupo) + 1\n",
        "            # Iterar sobre los grupos\n",
        "            for i in range(cantidad_grupos):\n",
        "                # Calcular el índice de inicio y fin para el grupo actual\n",
        "                inicio = i * tamano_grupo\n",
        "                fin = min((i + 1) * tamano_grupo, total_filas)\n",
        "\n",
        "                # Obtener el grupo actual\n",
        "                grupo = dataframe.iloc[inicio:fin]\n",
        "\n",
        "                # Verificar si el tamaño del grupo es menor a 672\n",
        "                if len(grupo) < tamano_grupo:\n",
        "                    # Arreglamos la serie de tiempo para que cumpla con 672\n",
        "                    grupo_completo = self.completeGroup(grupo,672)\n",
        "                    grupo_imputado = self.processData(grupo_completo)\n",
        "                    self.insertDataBase(grupo_imputado.iloc[:len(grupo)])\n",
        "\n",
        "                else:\n",
        "                    ### si armamos un grupo de 672 directamente pasamos a preprocesamiento del dataset\n",
        "                    grupo_imputado = self.processData(grupo)\n",
        "\n",
        "                    ### insertamos en base de datos\n",
        "                    self.insertDataBase(grupo_imputado)\n",
        "\n",
        "      def completeGroup(self,df,objetivo_filas):\n",
        "          # Calcular cuántas veces necesitas repetir el DataFrame\n",
        "          repeticiones = -(-objetivo_filas // len(df))  # Equivalente a math.ceil(objetivo_filas / len(df))\n",
        "          # Repetir el DataFrame\n",
        "          df_repetido = pd.concat([df] * repeticiones, ignore_index=True)\n",
        "          # Seleccionar las primeras objetivo_filas filas\n",
        "          df_completo = df_repetido.head(objetivo_filas)\n",
        "          return df_completo\n",
        "\n",
        "\n",
        "      def insertDataBase(self,df):\n",
        "          # Realizar una consulta SQL desde Python utilizando pyodbc\n",
        "            try:\n",
        "                 conexion = pyodbc.connect(self.conexion_str)\n",
        "\n",
        "                 # Crear un DataFrame (simulando que tienes un DataFrame llamado DF_BARRAS_ABB)\n",
        "\n",
        "                 # Crear un cursor para ejecutar consultas\n",
        "                 cursor = conexion.cursor()\n",
        "\n",
        "                 # Construir la consulta de inserción con todas las columnas\n",
        "                 consulta_insert = \"INSERT INTO dbo.SCADA_UNIFICADO (CIRCUITO, TIEMPO_AJUSTADO, IA,IB,IC,VA,VB,VC,P,Q,CONFIABILIDAD_IA,CONFIABILIDAD_IB,CONFIABILIDAD_IC,CONFIABILIDAD_VA,CONFIABILIDAD_VB,CONFIABILIDAD_VC,CONFIABILIDAD_P,CONFIABILIDAD_Q,SCADA) VALUES (?, ?, ?, ?, ?,?, ?, ?, ?, ?,?, ?, ?, ?, ?,?, ?, ?, ?)\"\n",
        "\n",
        "                 # Iterar sobre las filas del DataFrame e insertar cada fila en la base de datos\n",
        "                 for index, row in df.iterrows():\n",
        "                     cursor.execute(consulta_insert, tuple(row))\n",
        "\n",
        "                 # Confirmar los cambios en la base de datos\n",
        "                 conexion.commit()\n",
        "\n",
        "                 print(\"Datos insertados exitosamente en la tabla.\")\n",
        "            except pyodbc.Error as e:\n",
        "                 print(\"Error al conectar a la base de datos:\", e)\n",
        "            finally:\n",
        "                 conexion.close()\n",
        "\n",
        "\n",
        "      def processData(self,df):\n",
        "                \"\"\"\n",
        "                process data from a selected dataframe of a specific circuit\n",
        "                ----------------------------------------------------------------\n",
        "                df => dataframe\n",
        "                \"\"\"\n",
        "                ##########################################\n",
        "                ####### NORMALIZAMOS LOS DATOS ###########\n",
        "                ##########################################\n",
        "                name_circuit = df['CIRCUITO'].unique()[0]\n",
        "\n",
        "                lista_10 = []\n",
        "                lista_series_tiempo = []\n",
        "                Data_10 = []\n",
        "\n",
        "                df_matriz=df.values[:,1:8] ## eliminamos la columna del nombre del circuito que no nos interesa por el momento\n",
        "                df_matriz_total = df.values\n",
        "                # Tamaño de la serie de tiempo completa\n",
        "                tamaño_completo = df_matriz.shape[0]\n",
        "\n",
        "                # Tamaño deseado de las series segmentadas\n",
        "                tamaño_segmento = 672 ## SEGMENTADO POR SEMANA\n",
        "                channels = 7 ## CANTIDAD DE CANALES\n",
        "                # Número de series de tiempo\n",
        "                num_series = df_matriz.shape[1]\n",
        "\n",
        "                # Calcular la cantidad de series segmentadas\n",
        "                cantidad_series_segmentadas = tamaño_completo // tamaño_segmento\n",
        "                timesList = []\n",
        "\n",
        "                if (cantidad_series_segmentadas != 0):\n",
        "\n",
        "                    # OBTENEMOS LAS SERIES DE TIEMPO\n",
        "                    start = 0\n",
        "                    end = 0\n",
        "                    lista = []\n",
        "                    datos = np.zeros((cantidad_series_segmentadas,channels,tamaño_segmento))\n",
        "                    Label = []\n",
        "\n",
        "                    for i in range(1 , cantidad_series_segmentadas+1):\n",
        "                        start = end\n",
        "                        end = start + 672\n",
        "\n",
        "                        TIME = df_matriz[start:end,0]\n",
        "                        timesList.append(TIME)\n",
        "                        serie_ia = df_matriz[start:end,1]\n",
        "                        serie_ib = df_matriz[start:end,2]\n",
        "                        serie_ic = df_matriz[start:end,3]\n",
        "\n",
        "                        serie_va = df_matriz[start:end,4]\n",
        "                        serie_vb = df_matriz[start:end,5]\n",
        "                        serie_vc = df_matriz[start:end,6]\n",
        "\n",
        "\n",
        "                        datos[i-1,1,:] = serie_ia\n",
        "                        datos[i-1,2,:] = serie_ib\n",
        "                        datos[i-1,3,:] = serie_ic\n",
        "\n",
        "                        datos[i-1,4,:] = serie_va\n",
        "                        datos[i-1,5,:] = serie_vb\n",
        "                        datos[i-1,6,:] = serie_vc\n",
        "\n",
        "                    Label = [10] * cantidad_series_segmentadas\n",
        "                    lista_10.append([datos,Label])\n",
        "                    lista_series_tiempo.append([datos,Label])\n",
        "\n",
        "\n",
        "\n",
        "                Data_10 = lista_10[0][0]\n",
        "                Label_10 = lista_10[0][1]\n",
        "\n",
        "                for i in range (1,len(lista_10)):\n",
        "                  Data_10 = np.concatenate((Data_10, lista_10[i][0]), axis=0)\n",
        "\n",
        "                ###YA TENEMOS LA SERIE DE TIEMPO DE LA FORMA QUE REQUIERE EL MODELO EN DATA_10\n",
        "                ###AHORA APLICAMOS EL PREPROCESAMIENTO NECESARIO\n",
        "\n",
        "                Data = np.copy(Data_10[:,1:,:])\n",
        "                datos_normalizados = np.zeros_like(Data_10[:,1:,:], dtype=float)\n",
        "                Lista_nulos = []\n",
        "                Lista_reemplazo_z_score=[]\n",
        "                Lista_limites = np.zeros_like(Data[:,:,:2], dtype=float)\n",
        "\n",
        "                for i in range(Data.shape[0]):\n",
        "                  # Iterar a través de cada serie de tiempo en el conjunto de datos\n",
        "                  for j in range(Data.shape[1]):\n",
        "                      # Obtener la serie de tiempo actual\n",
        "                      \"\"\"\n",
        "                      MIRAR QUE LOS NULOS NO AFECTEN Y CONSEGUIR LOS INDICES DE LOS VALORES NULOS\n",
        "                      \"\"\"\n",
        "                      serie_tiempo = Data[i, j, :]\n",
        "                      index_null = np.where(np.isnan(serie_tiempo))\n",
        "                      Lista_nulos.append(index_null)### guardamos la lista de los indices de valores nulos por serie de tiempo\n",
        "                      mediana_arreglo = np.nanmedian(serie_tiempo) ## SIN TENER ENCUENTA LOS NULOS\n",
        "                      ### reemplazamos los nulos por la media momentaneamente para retirar\n",
        "                      serie_tiempo[index_null] = mediana_arreglo\n",
        "                      # Detectar valores atípicos en la serie de tiempo usando el método Z-score\n",
        "                      z_scores = np.abs(stats.zscore(serie_tiempo))\n",
        "                      # Definir un umbral para considerar valores atípicos (por ejemplo, 2 desviaciones estándar)\n",
        "                      umbral = 2.0\n",
        "                      index_reemplazada = z_scores > umbral\n",
        "                      index_reemplazada = np.where( index_reemplazada == True)[0]\n",
        "                      valores_reemplazada = serie_tiempo[index_reemplazada]\n",
        "                      Lista_reemplazo_z_score.append([index_reemplazada,valores_reemplazada])\n",
        "                      # Reemplazar valores atípicos por la mediana de la serie de tiempo\n",
        "                      serie_tiempo[z_scores > umbral] = mediana_arreglo\n",
        "                      #### NORMALIZAMOS LA SERIE DE TIEMPO\n",
        "                      min_ = np.min(serie_tiempo)\n",
        "                      max_ = np.max(serie_tiempo)\n",
        "                      if(min_ == max_):\n",
        "                        Lista_limites[i, j, :]=np.array([min_,max_])\n",
        "                        serie_tiempo = serie_tiempo/min_\n",
        "                        serie_tiempo[index_null] = min_ ### ubicamos los nulos como negativos\n",
        "                        # Almacenar la serie de tiempo normalizada en la matriz de datos normalizados\n",
        "\n",
        "                      else:\n",
        "                        Lista_limites[i, j, :]=np.array([min_,max_])\n",
        "                        serie_tiempo = (serie_tiempo - np.min(serie_tiempo)) / (np.max(serie_tiempo) - np.min(serie_tiempo))\n",
        "                        serie_tiempo[index_null] = -0.2 ### ubicamos los nulos como negativos\n",
        "                        # Almacenar la serie de tiempo normalizada en la matriz de datos normalizados\n",
        "\n",
        "                      datos_normalizados[i, j, :] = serie_tiempo\n",
        "\n",
        "                #### UNA VEZ NORMALIZADA LA SERIE DE TIEMPO LO QUE HACEMOS ES DECIDIR A QUE MODELO HACER LA INFERENCIA\n",
        "                #### obtenemos el modelo dependiendo del tipo de circuito\n",
        "                model,model_name = self.findModel(name_circuit)\n",
        "                ### hacemos la inferencia con el modelo\n",
        "                X_model = model.predict(datos_normalizados)\n",
        "                X_model = X_model.reshape((1, 6, 672))\n",
        "                #### Con los datos del modelo predicho sacamos reemplazamos cada serie de tiempo por sus nulos\n",
        "                r2_serie = []\n",
        "                for i in range(0,len(Lista_nulos)):\n",
        "                    ### reemplazamos los valores nulos por los imputados por el modelo\n",
        "                    if (Lista_limites[0,i,0] != Lista_limites[0,i,1]):\n",
        "                        datos_normalizados[0,i,Lista_nulos[i]] = X_model[0,i,Lista_nulos[i]]\n",
        "                        r2_serie.append(self.calc_r2(len(Lista_nulos[i]),model_name,i))\n",
        "                    else:\n",
        "                        r2_serie.append(1)\n",
        "                ### CALCULAMOS EL R2 DE LO OBTENIDO POR EL MODELO Y LA SERIE DE TIEMPO IMPUTADA CON LOS DATOS DEL MODELO\n",
        "\n",
        "                for i in range(0,len(Lista_nulos)):\n",
        "                    if (Lista_limites[0,i,0] != Lista_limites[0,i,1]):\n",
        "                        datos_normalizados[0,i,:]=(datos_normalizados[0,i,:] * (Lista_limites[0,i,1] - Lista_limites[0,i,0]) ) + Lista_limites[0,i,0]\n",
        "                        datos_normalizados[0,i,Lista_reemplazo_z_score[i][0]]=Lista_reemplazo_z_score[i][1]\n",
        "                    else:\n",
        "                        datos_normalizados[0,i,:]=datos_normalizados[0,i,:] * Lista_limites[0,i,1]\n",
        "                        datos_normalizados[0,i,Lista_reemplazo_z_score[i][0]]=Lista_reemplazo_z_score[i][1]\n",
        "\n",
        "                #### Restructuramos para devolver un dataframe con todos los datos imputados\n",
        "                ##CORRIENTES\n",
        "                df_matriz_total[:,2] = datos_normalizados[0,0] ## ia\n",
        "                df_matriz_total[:,3] = datos_normalizados[0,1] ## ib\n",
        "                df_matriz_total[:,4] = datos_normalizados[0,2] ## ic\n",
        "\n",
        "                ##VOLTAJES\n",
        "                df_matriz_total[:,5] = datos_normalizados[0,3] ## va\n",
        "                df_matriz_total[:,6] = datos_normalizados[0,4] ## vb\n",
        "                df_matriz_total[:,7] = datos_normalizados[0,5] ## vc\n",
        "\n",
        "\n",
        "                ### REEMPLAZAMOS LOS VALORES DE CONFIABILIDAD DEL DATO DE LOS VOLTAJES Y CORRIENTES IMPUTADOS\n",
        "                ##CORRIENTES\n",
        "                df_matriz_total[Lista_nulos[0],10] = [r2_serie[0]]*len(Lista_nulos[0]) ## ia\n",
        "                df_matriz_total[Lista_nulos[1],11] = [r2_serie[1]]*len(Lista_nulos[1]) ## ib\n",
        "                df_matriz_total[Lista_nulos[2],12] = [r2_serie[2]]*len(Lista_nulos[2]) ## ic\n",
        "\n",
        "                ##VOLTAJES\n",
        "                df_matriz_total[Lista_nulos[3],13] = [r2_serie[3]]*len(Lista_nulos[3]) ## va\n",
        "                df_matriz_total[Lista_nulos[4],14] = [r2_serie[4]]*len(Lista_nulos[4]) ## vb\n",
        "                df_matriz_total[Lista_nulos[5],15] = [r2_serie[5]]*len(Lista_nulos[5]) ## vc\n",
        "                \n",
        "                df.loc[:, :] = df_matriz_total\n",
        "                ##CALCULAMOS LAS POTENCIAS DE CADA FILA.\n",
        "                sign_data =  self.findLastRegister(df) ## obtenemos el signo de las potencias\n",
        "\n",
        "                if (sign_data != None) :\n",
        "                    df['P'].fillna( (sign_data[0]*(df['VA'] * df['IA'] * 0.9 + df['VB'] * df['IB'] * 0.9 + df['VC'] * df['IC'] * 0.9))/1000,inplace = True)\n",
        "                    df['Q'].fillna( (sign_data[1]*(df['VA'] * df['IA'] * 0.435 + df['VB'] * df['IB'] * 0.435 + df['VC'] * df['IC'] * 0.435))/1000,inplace = True)\n",
        "                else:\n",
        "                    df['P'].fillna( (df['VA'] * df['IA'] * 0.9 + df['VB'] * df['IB'] * 0.9 + df['VC'] * df['IC'] * 0.9)/1000,inplace = True)\n",
        "                    df['Q'].fillna( (df['VA'] * df['IA'] * 0.435 + df['VB'] * df['IB'] * 0.435 + df['VC'] * df['IC'] * 0.435)/1000,inplace = True)\n",
        "                \n",
        "                df['CONFIABILIDAD_P'] = df.apply(lambda row: row[['CONFIABILIDAD_IA', 'CONFIABILIDAD_IB', 'CONFIABILIDAD_IC', 'CONFIABILIDAD_VA', 'CONFIABILIDAD_VB', 'CONFIABILIDAD_VC']].mean() if pd.isna(row['CONFIABILIDAD_P']) else row['CONFIABILIDAD_P'], axis=1)\n",
        "                df['CONFIABILIDAD_Q'] = df.apply(lambda row: row[['CONFIABILIDAD_IA', 'CONFIABILIDAD_IB', 'CONFIABILIDAD_IC', 'CONFIABILIDAD_VA', 'CONFIABILIDAD_VB', 'CONFIABILIDAD_VC']].mean() if pd.isna(row['CONFIABILIDAD_Q']) else row['CONFIABILIDAD_Q'], axis=1)\n",
        "\n",
        "                return df\n",
        "\n",
        "\n",
        "      def calc_r2(self,cantidad_perdidos,modelo,variable):\n",
        "            \n",
        "            dictionario = {\n",
        "                '10':{\n",
        "                    '0':{\n",
        "                        'datos':[0.97,0.90,0.8,0.72,0.56,0.47],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '1':{\n",
        "                        'datos':[0.97,0.90,0.8,0.72,0.56,0.47],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '2':{\n",
        "                        'datos':[0.97,0.90,0.8,0.67,0.61,0.52],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '3':{\n",
        "                        'datos':[0.97,0.90,0.8,0.67,0.61,0.52],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '4':{\n",
        "                        'datos':[0.98,0.91,0.85,0.75,0.6,0.53],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '5':{\n",
        "                        'datos':[0.97,0.90,0.87,0.77,0.68,0.51],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    },\n",
        "                '3':{\n",
        "                    '0':{\n",
        "                        'datos':[0.98,0.93,0.87,0.63,0.45,0.38],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '1':{\n",
        "                        'datos':[0.99,0.93,0.88,0.67,0.44,0.37],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '2':{\n",
        "                        'datos':[0.98,0.93,0.87,0.68,0.46,0.41],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '3':{\n",
        "                        'datos':[0.96,0.90,0.8,0.57,0.41,0.13],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '4':{\n",
        "                        'datos':[0.98,0.91,0.8,0.57,0.31,0.18],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '5':{\n",
        "                        'datos':[0.97,0.88,0.82,0.58,0.33,0.21],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                },\n",
        "                '2':{\n",
        "                    '0':{\n",
        "                        'datos':[0.97,0.89,0.83,0.67,0.46,0.28],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '1':{\n",
        "                        'datos':[0.97,0.9,0.88,0.68,0.5,0.33],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '2':{\n",
        "                        'datos':[0.97,0.9,0.8,0.67,0.52,0.34],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '3':{\n",
        "                        'datos':[0.97,0.9,0.83,0.67,0.49,0.32],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '4':{\n",
        "                        'datos':[0.98,0.89,0.78,0.67,0.45,0.31],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '5':{\n",
        "                        'datos':[0.98,0.91,0.84,0.63,0.47,0.27],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                },\n",
        "                'GOOD':{\n",
        "                    '0':{\n",
        "                        'datos':[0.98,0.88,0.83,0.64,0.37,0.25],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '1':{\n",
        "                        'datos':[0.98,0.93,0.83,0.68,0.45,0.33],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '2':{\n",
        "                        'datos':[0.98,0.92,0.87,0.67,0.51,0.42],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '3':{\n",
        "                        'datos':[0.98,0.9,0.83,0.58,0.3,0.15],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '4':{\n",
        "                        'datos':[0.98,0.9,0.87,0.67,0.32,0.25],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                    '5':{\n",
        "                        'datos':[0.98,0.9,0.87,0.60,0.32,0.24],\n",
        "                        'limite':[50,100,200,300,400]\n",
        "                        },\n",
        "                }\n",
        "            }\n",
        "\n",
        "            dict_model = dictionario[modelo][str(variable)]\n",
        "\n",
        "            if (cantidad_perdidos <= dict_model['limite'][0]):\n",
        "                return dict_model['datos'][0]\n",
        "            elif (cantidad_perdidos <= dict_model['limite'][1]):\n",
        "                return dict_model['datos'][1]\n",
        "            elif (cantidad_perdidos <= dict_model['limite'][2]):\n",
        "                return dict_model['datos'][2]\n",
        "            elif (cantidad_perdidos <= dict_model['limite'][3]):\n",
        "                return dict_model['datos'][3]\n",
        "            elif (cantidad_perdidos <= dict_model['limite'][4]):\n",
        "                return dict_model['datos'][4]\n",
        "            else:\n",
        "                return dict_model['datos'][5]\n",
        "      \n",
        "\n",
        "      def findLastRegister(self,registros_asociados):\n",
        "\n",
        "            ###NO SE TENDRIA CERTEZA DEL FLUJO CON P=0 o Q=0\n",
        "            registros_filtrados = registros_asociados[(registros_asociados['P'].notna()) & (registros_asociados['P'] != 0) & (registros_asociados['Q'].notna()) & (registros_asociados['Q'] != 0)]\n",
        "            registros_ordenados = registros_filtrados.sort_values(by=['TIEMPO_AJUSTADO'], ascending=False)\n",
        "\n",
        "            if (registros_ordenados.shape[0]==0):\n",
        "               return None\n",
        "            else:\n",
        "                return [self.getSign(registros_ordenados.iloc[0].loc['P']),self.getSign(registros_ordenados.iloc[0].loc['Q'])]\n",
        "\n",
        "\n",
        "\n",
        "      def getSign(self,Number):\n",
        "          ## función para obtener el signo de un número\n",
        "          if ( float(Number) < 0 ):\n",
        "              return -1\n",
        "          else:\n",
        "              return 1\n",
        "\n",
        "\n",
        "      def getFiability(self):\n",
        "          \"\"\"\n",
        "          Model to get fiability\n",
        "          -----------------------------------\n",
        "          using the r coefficient related with the quantity\n",
        "          of loss values in the time serie.\n",
        "          \"\"\"\n",
        "\n",
        "          \n",
        "          pass\n",
        "\n",
        "\n",
        "      def findModel(self,name_model):\n",
        "          \"\"\"\n",
        "          function to find the specific model that is useful for the circuit\n",
        "          that we are evaluating\n",
        "          \"\"\"\n",
        "          Lista_cabezera = ['10','23','30','46','40']\n",
        "          Lista_red = ['2','3']\n",
        "\n",
        "          dictionary = {\n",
        "\n",
        "              '10' : self.MODEL_10,\n",
        "              '2' : self.MODEL_2,\n",
        "              '3' : self.MODEL_3,\n",
        "              '23' : self.MODEL_GOOD,\n",
        "              '30' : self.MODEL_GOOD,\n",
        "              '46' : self.MODEL_GOOD,\n",
        "              '40' : self.MODEL_GOOD\n",
        "\n",
        "          }\n",
        "\n",
        "          dictionary_2 = {\n",
        "\n",
        "              '10' : '10',\n",
        "              '2' : '2',\n",
        "              '3' : '3',\n",
        "              '23' : 'GOOD',\n",
        "              '30' : 'GOOD',\n",
        "              '46' : 'GOOD',\n",
        "              '40' : 'GOOD'\n",
        "\n",
        "          }\n",
        "\n",
        "          if (name_model[3:5] in Lista_cabezera):\n",
        "            return dictionary[name_model[3:5]],dictionary_2[name_model[3:5]]\n",
        "\n",
        "          elif (name_model[0] in Lista_red):\n",
        "            return dictionary[name_model[0]],dictionary_2[name_model[0]]\n",
        "          else:\n",
        "            return self.MODEL_ALL,'GOOD'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "VtdSbiqZLNBW",
        "outputId": "b15dc71d-eb26-4a81-a957-0ff430f6b015"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv('probe (1).csv',sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "hY7ovs1pHWDr"
      },
      "outputs": [],
      "source": [
        "#### CARGAMOS LOS MODELOS\n",
        "VAE_MODELS  = VaeModel(credenciales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loOoFtx7TtgZ",
        "outputId": "e4b9b624-d61a-448d-9597-a091cb150d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 334ms/step\n",
            "['2023-11-06 18:30:00' '2023-11-06 18:45:00' '2023-11-06 19:00:00'\n",
            " '2023-11-06 19:15:00' '2023-11-06 19:30:00' '2023-11-06 19:45:00'\n",
            " '2023-11-06 20:00:00' '2023-11-06 20:15:00' '2023-11-06 20:30:00'\n",
            " '2023-11-06 20:45:00' '2023-11-06 21:00:00' '2023-11-06 21:15:00'\n",
            " '2023-11-06 21:30:00' '2023-11-06 21:45:00' '2023-11-06 22:00:00'\n",
            " '2023-11-06 22:15:00' '2023-11-06 22:30:00' '2023-11-06 22:45:00'\n",
            " '2023-11-06 23:00:00' '2023-11-06 23:15:00' '2023-11-06 23:30:00'\n",
            " '2023-11-06 23:45:00' '2023-11-07 00:00:00' '2023-11-07 00:15:00'\n",
            " '2023-11-07 00:30:00' '2023-11-07 00:45:00' '2023-11-07 01:00:00'\n",
            " '2023-11-07 01:15:00' '2023-11-07 01:30:00' '2023-11-07 01:45:00'\n",
            " '2023-11-07 02:00:00' '2023-11-07 02:15:00' '2023-11-07 02:30:00'\n",
            " '2023-11-07 02:45:00' '2023-11-07 03:00:00' '2023-11-07 03:15:00'\n",
            " '2023-11-07 03:30:00' '2023-11-07 03:45:00' '2023-11-07 04:00:00'\n",
            " '2023-11-07 04:15:00' '2023-11-07 04:30:00' '2023-11-07 04:45:00'\n",
            " '2023-11-07 05:00:00' '2023-11-07 05:15:00' '2023-11-07 05:30:00'\n",
            " '2023-11-07 05:45:00' '2023-11-07 06:00:00' '2023-11-07 06:15:00'\n",
            " '2023-11-07 06:30:00' '2023-11-07 06:45:00' '2023-11-07 07:00:00'\n",
            " '2023-11-07 07:15:00' '2023-11-07 07:30:00' '2023-11-07 07:45:00'\n",
            " '2023-11-07 08:00:00' '2023-11-07 08:15:00' '2023-11-07 08:30:00'\n",
            " '2023-11-07 08:45:00' '2023-11-07 09:00:00' '2023-11-07 09:15:00'\n",
            " '2023-11-07 09:30:00' '2023-11-07 09:45:00' '2023-11-07 10:00:00'\n",
            " '2023-11-07 10:15:00' '2023-11-07 10:30:00' '2023-11-07 10:45:00'\n",
            " '2023-11-07 11:00:00' '2023-11-07 11:15:00' '2023-11-07 11:30:00'\n",
            " '2023-11-07 11:45:00' '2023-11-07 12:00:00' '2023-11-07 12:15:00'\n",
            " '2023-11-07 12:30:00' '2023-11-07 12:45:00' '2023-11-07 13:00:00'\n",
            " '2023-11-07 13:15:00' '2023-11-07 13:30:00' '2023-11-07 13:45:00'\n",
            " '2023-11-07 14:00:00' '2023-11-07 14:15:00' '2023-11-07 14:30:00'\n",
            " '2023-11-07 14:45:00' '2023-11-07 15:00:00' '2023-11-07 15:15:00'\n",
            " '2023-11-07 15:30:00' '2023-11-07 15:45:00' '2023-11-07 16:00:00'\n",
            " '2023-11-07 16:15:00' '2023-11-07 16:30:00' '2023-11-07 16:45:00'\n",
            " '2023-11-07 17:00:00' '2023-11-07 17:15:00' '2023-11-07 17:30:00'\n",
            " '2023-11-07 17:45:00' '2023-11-07 18:00:00' '2023-11-07 18:15:00'\n",
            " '2023-11-07 18:30:00' '2023-11-07 18:45:00' '2023-11-07 19:00:00'\n",
            " '2023-11-07 19:15:00' '2023-11-07 19:30:00' '2023-11-07 19:45:00'\n",
            " '2023-11-07 20:00:00' '2023-11-07 20:15:00' '2023-11-07 20:30:00'\n",
            " '2023-11-07 20:45:00' '2023-11-07 21:00:00' '2023-11-07 21:15:00'\n",
            " '2023-11-07 21:30:00' '2023-11-07 21:45:00' '2023-11-07 22:00:00'\n",
            " '2023-11-07 22:15:00' '2023-11-07 22:30:00' '2023-11-07 22:45:00'\n",
            " '2023-11-07 23:00:00' '2023-11-07 23:15:00' '2023-11-07 23:30:00'\n",
            " '2023-11-07 23:45:00' '2023-11-08 00:00:00' '2023-11-08 00:15:00'\n",
            " '2023-11-08 00:30:00' '2023-11-08 00:45:00' '2023-11-08 01:00:00'\n",
            " '2023-11-08 01:15:00' '2023-11-08 01:30:00' '2023-11-08 01:45:00'\n",
            " '2023-11-08 02:00:00' '2023-11-08 02:15:00' '2023-11-08 02:30:00'\n",
            " '2023-11-08 02:45:00' '2023-11-08 03:00:00' '2023-11-08 03:15:00'\n",
            " '2023-11-08 03:30:00' '2023-11-08 03:45:00' '2023-11-08 04:00:00'\n",
            " '2023-11-08 04:15:00' '2023-11-08 04:30:00' '2023-11-08 04:45:00'\n",
            " '2023-11-08 05:00:00' '2023-11-08 05:15:00' '2023-11-08 05:30:00'\n",
            " '2023-11-08 05:45:00' '2023-11-08 06:00:00' '2023-11-08 06:15:00'\n",
            " '2023-11-08 06:30:00' '2023-11-08 06:45:00' '2023-11-08 07:00:00'\n",
            " '2023-11-08 07:15:00' '2023-11-08 07:30:00' '2023-11-08 07:45:00'\n",
            " '2023-11-08 08:00:00' '2023-11-08 08:15:00' '2023-11-08 08:30:00'\n",
            " '2023-11-08 08:45:00' '2023-11-08 09:00:00' '2023-11-08 09:15:00'\n",
            " '2023-11-08 09:30:00' '2023-11-08 09:45:00' '2023-11-08 10:00:00'\n",
            " '2023-11-08 10:15:00' '2023-11-08 10:30:00' '2023-11-08 10:45:00'\n",
            " '2023-11-08 11:00:00' '2023-11-08 11:15:00' '2023-11-08 11:30:00'\n",
            " '2023-11-08 11:45:00' '2023-11-08 12:00:00' '2023-11-08 12:15:00'\n",
            " '2023-11-08 12:30:00' '2023-11-08 12:45:00' '2023-11-08 13:00:00'\n",
            " '2023-11-08 13:15:00' '2023-11-08 13:30:00' '2023-11-08 13:45:00'\n",
            " '2023-11-08 14:00:00' '2023-11-08 14:15:00' '2023-11-08 14:30:00'\n",
            " '2023-11-08 14:45:00' '2023-11-08 15:00:00' '2023-11-08 15:15:00'\n",
            " '2023-11-08 15:30:00' '2023-11-08 15:45:00' '2023-11-08 16:00:00'\n",
            " '2023-11-08 16:15:00' '2023-11-08 16:30:00' '2023-11-08 16:45:00'\n",
            " '2023-11-08 17:00:00' '2023-11-08 17:15:00' '2023-11-08 17:30:00'\n",
            " '2023-11-08 17:45:00' '2023-11-08 18:00:00' '2023-11-08 18:15:00'\n",
            " '2023-11-08 18:30:00' '2023-11-08 18:45:00' '2023-11-08 19:00:00'\n",
            " '2023-11-08 19:15:00' '2023-11-08 19:30:00' '2023-11-08 19:45:00'\n",
            " '2023-11-08 20:00:00' '2023-11-08 20:15:00' '2023-11-08 20:30:00'\n",
            " '2023-11-08 20:45:00' '2023-11-08 21:00:00' '2023-11-08 21:15:00'\n",
            " '2023-11-08 21:30:00' '2023-11-08 21:45:00' '2023-11-08 22:00:00'\n",
            " '2023-11-08 22:15:00' '2023-11-08 22:30:00' '2023-11-08 22:45:00'\n",
            " '2023-11-08 23:00:00' '2023-11-08 23:15:00' '2023-11-08 23:30:00'\n",
            " '2023-11-08 23:45:00' '2023-11-09 00:00:00' '2023-11-09 00:15:00'\n",
            " '2023-11-09 00:30:00' '2023-11-09 00:45:00' '2023-11-09 01:00:00'\n",
            " '2023-11-09 01:15:00' '2023-11-09 01:30:00' '2023-11-09 01:45:00'\n",
            " '2023-11-09 02:00:00' '2023-11-09 02:15:00' '2023-11-09 02:30:00'\n",
            " '2023-11-09 02:45:00' '2023-11-09 03:00:00' '2023-11-09 03:15:00'\n",
            " '2023-11-09 03:30:00' '2023-11-09 03:45:00' '2023-11-09 04:00:00'\n",
            " '2023-11-09 04:15:00' '2023-11-09 04:30:00' '2023-11-09 04:45:00'\n",
            " '2023-11-09 05:00:00' '2023-11-09 05:15:00' '2023-11-09 05:30:00'\n",
            " '2023-11-09 05:45:00' '2023-11-09 06:00:00' '2023-11-09 06:15:00'\n",
            " '2023-11-09 06:30:00' '2023-11-09 06:45:00' '2023-11-09 07:00:00'\n",
            " '2023-11-09 07:15:00' '2023-11-09 07:30:00' '2023-11-09 07:45:00'\n",
            " '2023-11-09 08:00:00' '2023-11-09 08:15:00' '2023-11-09 08:30:00'\n",
            " '2023-11-09 08:45:00' '2023-11-09 09:00:00' '2023-11-09 09:15:00'\n",
            " '2023-11-09 09:30:00' '2023-11-09 09:45:00' '2023-11-09 10:00:00'\n",
            " '2023-11-09 10:15:00' '2023-11-09 10:30:00' '2023-11-09 10:45:00'\n",
            " '2023-11-09 11:00:00' '2023-11-09 11:15:00' '2023-11-09 11:30:00'\n",
            " '2023-11-09 11:45:00' '2023-11-09 12:00:00' '2023-11-09 12:15:00'\n",
            " '2023-11-09 12:30:00' '2023-11-09 12:45:00' '2023-11-09 13:00:00'\n",
            " '2023-11-09 13:15:00' '2023-11-09 13:30:00' '2023-11-09 13:45:00'\n",
            " '2023-11-09 14:00:00' '2023-11-09 14:15:00' '2023-11-09 14:30:00'\n",
            " '2023-11-09 14:45:00' '2023-11-09 15:00:00' '2023-11-09 15:15:00'\n",
            " '2023-11-09 15:30:00' '2023-11-09 15:45:00' '2023-11-09 16:00:00'\n",
            " '2023-11-09 16:15:00' '2023-11-09 16:30:00' '2023-11-09 16:45:00'\n",
            " '2023-11-09 17:00:00' '2023-11-09 17:15:00' '2023-11-09 17:30:00'\n",
            " '2023-11-09 17:45:00' '2023-11-09 18:00:00' '2023-11-09 18:15:00'\n",
            " '2023-11-09 18:30:00' '2023-11-09 18:45:00' '2023-11-09 19:00:00'\n",
            " '2023-11-09 19:15:00' '2023-11-09 19:30:00' '2023-11-09 19:45:00'\n",
            " '2023-11-09 20:00:00' '2023-11-09 20:15:00' '2023-11-09 20:30:00'\n",
            " '2023-11-09 20:45:00' '2023-11-09 21:00:00' '2023-11-09 21:15:00'\n",
            " '2023-11-09 21:30:00' '2023-11-09 21:45:00' '2023-11-09 22:00:00'\n",
            " '2023-11-09 22:15:00' '2023-11-09 22:30:00' '2023-11-09 22:45:00'\n",
            " '2023-11-09 23:00:00' '2023-11-09 23:15:00' '2023-11-09 23:30:00'\n",
            " '2023-11-09 23:45:00' '2023-11-10 00:00:00' '2023-11-10 00:15:00'\n",
            " '2023-11-10 00:30:00' '2023-11-10 00:45:00' '2023-11-10 01:00:00'\n",
            " '2023-11-10 01:15:00' '2023-11-10 01:30:00' '2023-11-10 01:45:00'\n",
            " '2023-11-10 02:00:00' '2023-11-10 02:15:00' '2023-11-10 02:30:00'\n",
            " '2023-11-10 02:45:00' '2023-11-10 03:00:00' '2023-11-10 03:15:00'\n",
            " '2023-11-10 03:30:00' '2023-11-10 03:45:00' '2023-11-10 04:00:00'\n",
            " '2023-11-10 04:15:00' '2023-11-10 04:30:00' '2023-11-10 04:45:00'\n",
            " '2023-11-10 05:00:00' '2023-11-10 05:15:00' '2023-11-10 05:30:00'\n",
            " '2023-11-10 05:45:00' '2023-11-10 06:00:00' '2023-11-10 06:15:00'\n",
            " '2023-11-10 06:30:00' '2023-11-10 06:45:00' '2023-11-10 07:00:00'\n",
            " '2023-11-10 07:15:00' '2023-11-10 07:30:00' '2023-11-10 07:45:00'\n",
            " '2023-11-10 08:00:00' '2023-11-10 08:15:00' '2023-11-10 08:30:00'\n",
            " '2023-11-10 08:45:00' '2023-11-10 09:00:00' '2023-11-10 09:15:00'\n",
            " '2023-11-10 09:30:00' '2023-11-10 09:45:00' '2023-11-10 10:00:00'\n",
            " '2023-11-10 10:15:00' '2023-11-10 10:30:00' '2023-11-10 10:45:00'\n",
            " '2023-11-10 11:00:00' '2023-11-10 11:15:00' '2023-11-10 11:30:00'\n",
            " '2023-11-10 11:45:00' '2023-11-10 12:00:00' '2023-11-10 12:15:00'\n",
            " '2023-11-10 12:30:00' '2023-11-10 12:45:00' '2023-11-10 13:00:00'\n",
            " '2023-11-10 13:15:00' '2023-11-10 13:30:00' '2023-11-10 13:45:00'\n",
            " '2023-11-10 14:00:00' '2023-11-10 14:15:00' '2023-11-10 14:30:00'\n",
            " '2023-11-10 14:45:00' '2023-11-10 15:00:00' '2023-11-10 15:15:00'\n",
            " '2023-11-10 15:30:00' '2023-11-10 15:45:00' '2023-11-10 16:00:00'\n",
            " '2023-11-10 16:15:00' '2023-11-10 16:30:00' '2023-11-10 16:45:00'\n",
            " '2023-11-10 17:00:00' '2023-11-10 17:15:00' '2023-11-10 17:30:00'\n",
            " '2023-11-10 17:45:00' '2023-11-10 18:00:00' '2023-11-10 18:15:00'\n",
            " '2023-11-10 18:30:00' '2023-11-10 18:45:00' '2023-11-10 19:00:00'\n",
            " '2023-11-10 19:15:00' '2023-11-10 19:30:00' '2023-11-10 19:45:00'\n",
            " '2023-11-10 20:00:00' '2023-11-10 20:15:00' '2023-11-10 20:30:00'\n",
            " '2023-11-10 20:45:00' '2023-11-10 21:00:00' '2023-11-10 21:15:00'\n",
            " '2023-11-10 21:30:00' '2023-11-10 21:45:00' '2023-11-10 22:00:00'\n",
            " '2023-11-10 22:15:00' '2023-11-10 22:30:00' '2023-11-10 22:45:00'\n",
            " '2023-11-10 23:00:00' '2023-11-10 23:15:00' '2023-11-10 23:30:00'\n",
            " '2023-11-10 23:45:00' '2023-11-11 00:00:00' '2023-11-11 00:15:00'\n",
            " '2023-11-11 00:30:00' '2023-11-11 00:45:00' '2023-11-11 01:00:00'\n",
            " '2023-11-11 01:15:00' '2023-11-11 01:30:00' '2023-11-11 01:45:00'\n",
            " '2023-11-11 02:00:00' '2023-11-11 02:15:00' '2023-11-11 02:30:00'\n",
            " '2023-11-11 02:45:00' '2023-11-11 03:00:00' '2023-11-11 03:15:00'\n",
            " '2023-11-11 03:30:00' '2023-11-11 03:45:00' '2023-11-11 04:00:00'\n",
            " '2023-11-11 04:15:00' '2023-11-11 04:30:00' '2023-11-11 04:45:00'\n",
            " '2023-11-11 05:00:00' '2023-11-11 05:15:00' '2023-11-11 05:30:00'\n",
            " '2023-11-11 05:45:00' '2023-11-11 06:00:00' '2023-11-11 06:15:00'\n",
            " '2023-11-11 06:30:00' '2023-11-11 06:45:00' '2023-11-11 07:00:00'\n",
            " '2023-11-11 07:15:00' '2023-11-11 07:30:00' '2023-11-11 07:45:00'\n",
            " '2023-11-11 08:00:00' '2023-11-11 08:15:00' '2023-11-11 08:30:00'\n",
            " '2023-11-11 08:45:00' '2023-11-11 09:00:00' '2023-11-11 09:15:00'\n",
            " '2023-11-11 09:30:00' '2023-11-11 09:45:00' '2023-11-11 10:00:00'\n",
            " '2023-11-11 10:15:00' '2023-11-11 10:30:00' '2023-11-11 10:45:00'\n",
            " '2023-11-11 11:00:00' '2023-11-11 11:15:00' '2023-11-11 11:30:00'\n",
            " '2023-11-11 11:45:00' '2023-11-11 12:00:00' '2023-11-11 12:15:00'\n",
            " '2023-11-11 12:30:00' '2023-11-11 12:45:00' '2023-11-11 13:00:00'\n",
            " '2023-11-11 13:15:00' '2023-11-11 13:30:00' '2023-11-11 13:45:00'\n",
            " '2023-11-11 14:00:00' '2023-11-11 14:15:00' '2023-11-11 14:30:00'\n",
            " '2023-11-11 14:45:00' '2023-11-11 15:00:00' '2023-11-11 15:15:00'\n",
            " '2023-11-11 15:30:00' '2023-11-11 15:45:00' '2023-11-11 16:00:00'\n",
            " '2023-11-11 16:15:00' '2023-11-11 16:30:00' '2023-11-11 16:45:00'\n",
            " '2023-11-11 17:00:00' '2023-11-11 17:15:00' '2023-11-11 17:30:00'\n",
            " '2023-11-11 17:45:00' '2023-11-11 18:00:00' '2023-11-11 18:15:00'\n",
            " '2023-11-11 18:30:00' '2023-11-11 18:45:00' '2023-11-11 19:00:00'\n",
            " '2023-11-11 19:15:00' '2023-11-11 19:30:00' '2023-11-11 19:45:00'\n",
            " '2023-11-11 20:00:00' '2023-11-11 20:15:00' '2023-11-11 20:30:00'\n",
            " '2023-11-11 20:45:00' '2023-11-11 21:00:00' '2023-11-11 21:15:00'\n",
            " '2023-11-11 21:30:00' '2023-11-11 21:45:00' '2023-11-11 22:00:00'\n",
            " '2023-11-11 22:15:00' '2023-11-11 22:30:00' '2023-11-11 22:45:00'\n",
            " '2023-11-11 23:00:00' '2023-11-11 23:15:00' '2023-11-11 23:30:00'\n",
            " '2023-11-11 23:45:00' '2023-11-12 00:00:00' '2023-11-12 00:15:00'\n",
            " '2023-11-12 00:30:00' '2023-11-12 00:45:00' '2023-11-12 01:00:00'\n",
            " '2023-11-12 01:15:00' '2023-11-12 01:30:00' '2023-11-12 01:45:00'\n",
            " '2023-11-12 02:00:00' '2023-11-12 02:15:00' '2023-11-12 02:30:00'\n",
            " '2023-11-12 02:45:00' '2023-11-12 03:00:00' '2023-11-12 03:15:00'\n",
            " '2023-11-12 03:30:00' '2023-11-12 03:45:00' '2023-11-12 04:00:00'\n",
            " '2023-11-12 04:15:00' '2023-11-12 04:30:00' '2023-11-12 04:45:00'\n",
            " '2023-11-12 05:00:00' '2023-11-12 05:15:00' '2023-11-12 05:30:00'\n",
            " '2023-11-12 05:45:00' '2023-11-12 06:00:00' '2023-11-12 06:15:00'\n",
            " '2023-11-12 06:30:00' '2023-11-12 06:45:00' '2023-11-12 07:00:00'\n",
            " '2023-11-12 07:15:00' '2023-11-12 07:30:00' '2023-11-12 07:45:00'\n",
            " '2023-11-12 08:00:00' '2023-11-12 08:15:00' '2023-11-12 08:30:00'\n",
            " '2023-11-12 08:45:00' '2023-11-12 09:00:00' '2023-11-12 09:15:00'\n",
            " '2023-11-12 09:30:00' '2023-11-12 09:45:00' '2023-11-12 10:00:00'\n",
            " '2023-11-12 10:15:00' '2023-11-12 10:30:00' '2023-11-12 10:45:00'\n",
            " '2023-11-12 11:00:00' '2023-11-12 11:15:00' '2023-11-12 11:30:00'\n",
            " '2023-11-12 11:45:00' '2023-11-12 12:00:00' '2023-11-12 12:15:00'\n",
            " '2023-11-12 12:30:00' '2023-11-12 12:45:00' '2023-11-12 13:00:00'\n",
            " '2023-11-12 13:15:00' '2023-11-12 13:30:00' '2023-11-12 13:45:00'\n",
            " '2023-11-12 14:00:00' '2023-11-12 14:15:00' '2023-11-12 14:30:00'\n",
            " '2023-11-12 14:45:00' '2023-11-12 15:00:00' '2023-11-12 15:15:00'\n",
            " '2023-11-12 15:30:00' '2023-11-12 15:45:00' '2023-11-12 16:00:00'\n",
            " '2023-11-12 16:15:00' '2023-11-12 16:30:00' '2023-11-12 16:45:00'\n",
            " '2023-11-12 17:00:00' '2023-11-12 17:15:00' '2023-11-12 17:30:00'\n",
            " '2023-11-12 17:45:00' '2023-11-12 18:00:00' '2023-11-12 18:15:00'\n",
            " '2023-11-12 18:30:00' '2023-11-12 18:45:00' '2023-11-12 19:00:00'\n",
            " '2023-11-12 19:15:00' '2023-11-12 19:30:00' '2023-11-12 19:45:00'\n",
            " '2023-11-12 20:00:00' '2023-11-12 20:15:00' '2023-11-12 20:30:00'\n",
            " '2023-11-12 20:45:00' '2023-11-12 21:00:00' '2023-11-12 21:15:00'\n",
            " '2023-11-12 21:30:00' '2023-11-12 21:45:00' '2023-11-12 22:00:00'\n",
            " '2023-11-12 22:15:00' '2023-11-12 22:30:00' '2023-11-12 22:45:00'\n",
            " '2023-11-12 23:00:00' '2023-11-12 23:15:00' '2023-11-12 23:30:00'\n",
            " '2023-11-12 23:45:00' '2023-11-13 00:00:00' '2023-11-13 00:15:00'\n",
            " '2023-11-13 00:30:00' '2023-11-13 00:45:00' '2023-11-13 01:00:00'\n",
            " '2023-11-13 01:15:00' '2023-11-13 01:30:00' '2023-11-13 01:45:00'\n",
            " '2023-11-13 02:00:00' '2023-11-13 02:15:00' '2023-11-13 02:30:00'\n",
            " '2023-11-13 02:45:00' '2023-11-13 03:00:00' '2023-11-13 03:15:00'\n",
            " '2023-11-13 03:30:00' '2023-11-13 03:45:00' '2023-11-13 04:00:00'\n",
            " '2023-11-13 04:15:00' '2023-11-13 04:30:00' '2023-11-13 04:45:00'\n",
            " '2023-11-13 05:00:00' '2023-11-13 05:15:00' '2023-11-13 05:30:00'\n",
            " '2023-11-13 05:45:00' '2023-11-13 06:00:00' '2023-11-13 06:15:00'\n",
            " '2023-11-13 06:30:00' '2023-11-13 06:45:00' '2023-11-13 07:00:00'\n",
            " '2023-11-13 07:15:00' '2023-11-13 07:30:00' '2023-11-13 07:45:00'\n",
            " '2023-11-13 08:00:00' '2023-11-13 08:15:00' '2023-11-13 08:30:00'\n",
            " '2023-11-13 08:45:00' '2023-11-13 09:00:00' '2023-11-13 09:15:00'\n",
            " '2023-11-13 09:30:00' '2023-11-13 09:45:00' '2023-11-13 10:00:00'\n",
            " '2023-11-13 10:15:00' '2023-11-13 10:30:00' '2023-11-13 10:45:00'\n",
            " '2023-11-13 11:00:00' '2023-11-13 11:15:00' '2023-11-13 11:30:00'\n",
            " '2023-11-13 11:45:00' '2023-11-13 12:00:00' '2023-11-13 12:15:00'\n",
            " '2023-11-13 12:30:00' '2023-11-13 12:45:00' '2023-11-13 13:00:00'\n",
            " '2023-11-13 13:15:00' '2023-11-13 13:30:00' '2023-11-13 13:45:00'\n",
            " '2023-11-13 14:00:00' '2023-11-13 14:15:00' '2023-11-13 14:30:00'\n",
            " '2023-11-13 14:45:00' '2023-11-13 15:00:00' '2023-11-13 15:15:00'\n",
            " '2023-11-13 15:30:00' '2023-11-13 15:45:00' '2023-11-13 16:00:00'\n",
            " '2023-11-13 16:15:00' '2023-11-13 16:30:00' '2023-11-13 16:45:00'\n",
            " '2023-11-13 17:00:00' '2023-11-13 17:15:00' '2023-11-13 17:30:00'\n",
            " '2023-11-13 17:45:00' '2023-11-13 18:00:00' '2023-11-13 18:15:00']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_31508\\2777131371.py:495: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['P'].fillna( (sign_data[0]*(df['VA'] * df['IA'] * 0.9 + df['VB'] * df['IB'] * 0.9 + df['VC'] * df['IC'] * 0.9))/1000,inplace = True)\n",
            "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_31508\\2777131371.py:496: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Q'].fillna( (sign_data[1]*(df['VA'] * df['IA'] * 0.435 + df['VB'] * df['IB'] * 0.435 + df['VC'] * df['IC'] * 0.435))/1000,inplace = True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos insertados exitosamente en la tabla.\n"
          ]
        }
      ],
      "source": [
        "VAE_MODELS.model_iteration(dataframe)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
